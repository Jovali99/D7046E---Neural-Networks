{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks and Learning Machines\n",
    "## Exercise 0 - Getting started with Python, Jupyter, and PyTorch\n",
    "\n",
    "**This exercise shouldn't be submitted and will not be graded**\n",
    "\n",
    "In this exercise we will do some simple programming tasks with neural networks with the aim to a) introduce you to the programming tools and libraries we'll be using and b) teach you some neural networks fundamentals and test your understanding of them.\n",
    "\n",
    "In this exercise you will use a collection of perceptrons (i.e. a single-layer neural network) to classify the digits shown on a seven-segment display.\n",
    "\n",
    "First we will implement the perceptrons using numpy and set the weights of those perceptrons by hand (without any actual machine learning) to classify the digits. This task will hopefully teach you the inner workings of individual artificial neurons as well as how changing weights and biases can make a network of neurons learn.\n",
    "\n",
    "Second we will implement the same single-layer neural network in PyTorch and try to use machine learning to learn the correct weights. This task will hopefully teach you how to do basic network training in PyTorch.\n",
    "\n",
    "Finally we will move away from the seven-segment display and try to train a larger artificial neural network (ANN) to tackle the MNIST dataset of handwritten digits. This task will hopefully prepare you to take on larger machine learning tasks with Pytorch as well as give you some insight into how machine learning can be used to solve problems that are diffcult (if not impossible) to program exactly right.\n",
    "\n",
    "Before you start this exercise it is recommended that you read the \"Getting ready for the Exercises\" PDF that can be found on Canvas. The first cell below will check if everything seems to be installed correctly, if not, first check the PDF and then, if the problem persists, contact a TA.\n",
    "\n",
    "Press the CONTROL + ENTER keys to run a selected block of code or text. Double click text blocks to edit them and press CONTROL + ENTER to turn it back into text. Whenever you've updated some code in a cell you need to run it again to execute those changes. If the changes in that cell is used in another cell you need to run that cell again as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The installation seems to be working!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "print('The installation seems to be working!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seven-segment display\n",
    "\n",
    "A seven-segment display [https://en.wikipedia.org/wiki/Seven-segment_display] can be used to display the different digits by turning the different segments (A,B,C,D,E,F) on or off. Your task is to design ten different perceptrons (which together make a single-layer neural network) that recognizes the ten different digits (0,1,2,3, ... ,9) represented by a seven-segment display. The input to each perceptron will be a vector {A,B,C,D,E,F} where A is 1 if segment A is turned on and 0 otherwise (and the same for all the other segments).\n",
    "\n",
    "![Seven Segment Display](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ed/7_Segment_Display_with_Labeled_Segments.svg/225px-7_Segment_Display_with_Labeled_Segments.svg.png)\n",
    "\n",
    "This means that for each digit you should create a perceptron and to check which digit the network thinks is displayed we input the vector {A,B,C,D,E,F} to each perceptron and then we check which perceptron gives the greatest output value. For each digit (0 to 9) the corresponding perceptron shoud have the greatest output value.\n",
    "\n",
    "For example we want to input 2 into the network. The digit two corresponds to the vector {1,1,0,1,1,0,1} so we input that into each perceptron. Then we check the output value of each perceptron and see which one is greatest. If the perceptron at index 2 (the third pereptron since we have a perceptron for 0 as well) gives the greatest output then our networks is working for the input 2. Now we finally need to check the other digits as well.\n",
    "\n",
    "\n",
    "For this task we use numpy rather than PyTorch.\n",
    "\n",
    "After completing this exercise you should understand how an artificial neural network unit (like the perceptron) produces one scalar output from multiple input values, and how the weights and biases of determine the relation between input and output values.\n",
    "\n",
    "**Exercise:** Complete the input vectors, the weight vectors, and biases. Then update the prediction calculation (forward pass) to include bias in the calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit 5 corresponds to the vector [1 0 1 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "## First we need to define all the vectors corresponding to the various digits and add them to a list for easy access\n",
    "# Please finish the list of digit vectors\n",
    "x = [\n",
    "    numpy.array([1,1,1,1,1,1,0]), # 0\n",
    "    numpy.array([0,1,1,0,0,0,0]), # 1\n",
    "    numpy.array([1,1,0,1,1,0,1]), # 2\n",
    "    numpy.array([1,1,1,1,0,0,1]), # 3\n",
    "    numpy.array([0,1,1,0,0,1,1]), # 4\n",
    "    numpy.array([1,0,1,1,0,1,1]), # 5\n",
    "    numpy.array([0,0,1,1,1,1,1]), # 6\n",
    "    numpy.array([1,1,1,0,0,0,0]), # 7\n",
    "    numpy.array([1,1,1,1,1,1,1]), # 8\n",
    "    numpy.array([1,1,1,0,0,1,1]), # 9\n",
    "]\n",
    "\n",
    "# And we print one of the vectors to show you how to get a specific vector\n",
    "print(f'Digit 5 corresponds to the vector {x[5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Second we need to create ten perceptron with weights and biases\n",
    "# You also need to figure out which weights and biases to use for each perceptron\n",
    "# We've already created some of the perceptrons for you, but you need to create the rest\n",
    "# While we're using integers for our weight you can use floating point numbers (real numbers) as well if you want\n",
    "\n",
    "weights = [\n",
    "    numpy.array([1,1,1,1,1,1,-1]), # 0\n",
    "    numpy.array([-1,1,1,-1,-1,-1,-1]), # 1\n",
    "    numpy.array([1,1,-1,1,1,-1,1]), # 2\n",
    "    numpy.array([1,1,1,1,-1,-1,1]), # 3\n",
    "    numpy.array([-1,1,1,-1,-1,1,1]), # 4\n",
    "    numpy.array([1,-1,1,1,-1,1,1]), # 5\n",
    "    numpy.array([0,0,1,1,1,1,1]), # 6\n",
    "    numpy.array([1,1,1,-1,-1,-1,-1]), # 7\n",
    "    numpy.array([1,1,1,1,1,1,1]), # 8\n",
    "    numpy.array([1,1,1,-1,-1,1,1]), # 9\n",
    "    \n",
    "]\n",
    "\n",
    "biases = [\n",
    "    0, # 0\n",
    "    1, # 1\n",
    "    0.1, # 2\n",
    "    0.3, # 3\n",
    "    0.5, # 4\n",
    "    0.2, # 5\n",
    "    0, # 6\n",
    "    0.8, # 7\n",
    "    0, # 8\n",
    "    0.4, # 9\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1  1  1  1 -1] shape w :  (7,)\n",
      "[1 1 1 1 1 1 0] shape vec :  (7,)\n",
      "[-1  1  1 -1 -1 -1 -1] shape w :  (7,)\n",
      "[1 1 1 1 1 1 0] shape vec :  (7,)\n",
      "[ 1  1 -1  1  1 -1  1] shape w :  (7,)\n",
      "[1 1 1 1 1 1 0] shape vec :  (7,)\n",
      "[ 1  1  1  1 -1 -1  1] shape w :  (7,)\n",
      "[1 1 1 1 1 1 0] shape vec :  (7,)\n",
      "[-1  1  1 -1 -1  1  1] shape w :  (7,)\n",
      "[1 1 1 1 1 1 0] shape vec :  (7,)\n",
      "[ 1 -1  1  1 -1  1  1] shape w :  (7,)\n",
      "[1 1 1 1 1 1 0] shape vec :  (7,)\n",
      "[0 0 1 1 1 1 1] shape w :  (7,)\n",
      "[1 1 1 1 1 1 0] shape vec :  (7,)\n",
      "[ 1  1  1 -1 -1 -1 -1] shape w :  (7,)\n",
      "[1 1 1 1 1 1 0] shape vec :  (7,)\n",
      "[1 1 1 1 1 1 1] shape w :  (7,)\n",
      "[1 1 1 1 1 1 0] shape vec :  (7,)\n",
      "[ 1  1  1 -1 -1  1  1] shape w :  (7,)\n",
      "[1 1 1 1 1 1 0] shape vec :  (7,)\n",
      "Digit 0 was predicted to be 9\n",
      "[ 1  1  1  1  1  1 -1] shape w :  (7,)\n",
      "[0 1 1 0 0 0 0] shape vec :  (7,)\n",
      "[-1  1  1 -1 -1 -1 -1] shape w :  (7,)\n",
      "[0 1 1 0 0 0 0] shape vec :  (7,)\n",
      "[ 1  1 -1  1  1 -1  1] shape w :  (7,)\n",
      "[0 1 1 0 0 0 0] shape vec :  (7,)\n",
      "[ 1  1  1  1 -1 -1  1] shape w :  (7,)\n",
      "[0 1 1 0 0 0 0] shape vec :  (7,)\n",
      "[-1  1  1 -1 -1  1  1] shape w :  (7,)\n",
      "[0 1 1 0 0 0 0] shape vec :  (7,)\n",
      "[ 1 -1  1  1 -1  1  1] shape w :  (7,)\n",
      "[0 1 1 0 0 0 0] shape vec :  (7,)\n",
      "[0 0 1 1 1 1 1] shape w :  (7,)\n",
      "[0 1 1 0 0 0 0] shape vec :  (7,)\n",
      "[ 1  1  1 -1 -1 -1 -1] shape w :  (7,)\n",
      "[0 1 1 0 0 0 0] shape vec :  (7,)\n",
      "[1 1 1 1 1 1 1] shape w :  (7,)\n",
      "[0 1 1 0 0 0 0] shape vec :  (7,)\n",
      "[ 1  1  1 -1 -1  1  1] shape w :  (7,)\n",
      "[0 1 1 0 0 0 0] shape vec :  (7,)\n",
      "Digit 1 was predicted to be 9\n",
      "[ 1  1  1  1  1  1 -1] shape w :  (7,)\n",
      "[1 1 0 1 1 0 1] shape vec :  (7,)\n",
      "[-1  1  1 -1 -1 -1 -1] shape w :  (7,)\n",
      "[1 1 0 1 1 0 1] shape vec :  (7,)\n",
      "[ 1  1 -1  1  1 -1  1] shape w :  (7,)\n",
      "[1 1 0 1 1 0 1] shape vec :  (7,)\n",
      "[ 1  1  1  1 -1 -1  1] shape w :  (7,)\n",
      "[1 1 0 1 1 0 1] shape vec :  (7,)\n",
      "[-1  1  1 -1 -1  1  1] shape w :  (7,)\n",
      "[1 1 0 1 1 0 1] shape vec :  (7,)\n",
      "[ 1 -1  1  1 -1  1  1] shape w :  (7,)\n",
      "[1 1 0 1 1 0 1] shape vec :  (7,)\n",
      "[0 0 1 1 1 1 1] shape w :  (7,)\n",
      "[1 1 0 1 1 0 1] shape vec :  (7,)\n",
      "[ 1  1  1 -1 -1 -1 -1] shape w :  (7,)\n",
      "[1 1 0 1 1 0 1] shape vec :  (7,)\n",
      "[1 1 1 1 1 1 1] shape w :  (7,)\n",
      "[1 1 0 1 1 0 1] shape vec :  (7,)\n",
      "[ 1  1  1 -1 -1  1  1] shape w :  (7,)\n",
      "[1 1 0 1 1 0 1] shape vec :  (7,)\n",
      "Digit 2 was predicted to be 9\n",
      "[ 1  1  1  1  1  1 -1] shape w :  (7,)\n",
      "[1 1 1 1 0 0 1] shape vec :  (7,)\n",
      "[-1  1  1 -1 -1 -1 -1] shape w :  (7,)\n",
      "[1 1 1 1 0 0 1] shape vec :  (7,)\n",
      "[ 1  1 -1  1  1 -1  1] shape w :  (7,)\n",
      "[1 1 1 1 0 0 1] shape vec :  (7,)\n",
      "[ 1  1  1  1 -1 -1  1] shape w :  (7,)\n",
      "[1 1 1 1 0 0 1] shape vec :  (7,)\n",
      "[-1  1  1 -1 -1  1  1] shape w :  (7,)\n",
      "[1 1 1 1 0 0 1] shape vec :  (7,)\n",
      "[ 1 -1  1  1 -1  1  1] shape w :  (7,)\n",
      "[1 1 1 1 0 0 1] shape vec :  (7,)\n",
      "[0 0 1 1 1 1 1] shape w :  (7,)\n",
      "[1 1 1 1 0 0 1] shape vec :  (7,)\n",
      "[ 1  1  1 -1 -1 -1 -1] shape w :  (7,)\n",
      "[1 1 1 1 0 0 1] shape vec :  (7,)\n",
      "[1 1 1 1 1 1 1] shape w :  (7,)\n",
      "[1 1 1 1 0 0 1] shape vec :  (7,)\n",
      "[ 1  1  1 -1 -1  1  1] shape w :  (7,)\n",
      "[1 1 1 1 0 0 1] shape vec :  (7,)\n",
      "Digit 3 was predicted to be 9\n",
      "[ 1  1  1  1  1  1 -1] shape w :  (7,)\n",
      "[0 1 1 0 0 1 1] shape vec :  (7,)\n",
      "[-1  1  1 -1 -1 -1 -1] shape w :  (7,)\n",
      "[0 1 1 0 0 1 1] shape vec :  (7,)\n",
      "[ 1  1 -1  1  1 -1  1] shape w :  (7,)\n",
      "[0 1 1 0 0 1 1] shape vec :  (7,)\n",
      "[ 1  1  1  1 -1 -1  1] shape w :  (7,)\n",
      "[0 1 1 0 0 1 1] shape vec :  (7,)\n",
      "[-1  1  1 -1 -1  1  1] shape w :  (7,)\n",
      "[0 1 1 0 0 1 1] shape vec :  (7,)\n",
      "[ 1 -1  1  1 -1  1  1] shape w :  (7,)\n",
      "[0 1 1 0 0 1 1] shape vec :  (7,)\n",
      "[0 0 1 1 1 1 1] shape w :  (7,)\n",
      "[0 1 1 0 0 1 1] shape vec :  (7,)\n",
      "[ 1  1  1 -1 -1 -1 -1] shape w :  (7,)\n",
      "[0 1 1 0 0 1 1] shape vec :  (7,)\n",
      "[1 1 1 1 1 1 1] shape w :  (7,)\n",
      "[0 1 1 0 0 1 1] shape vec :  (7,)\n",
      "[ 1  1  1 -1 -1  1  1] shape w :  (7,)\n",
      "[0 1 1 0 0 1 1] shape vec :  (7,)\n",
      "Digit 4 was predicted to be 9\n",
      "[ 1  1  1  1  1  1 -1] shape w :  (7,)\n",
      "[1 0 1 1 0 1 1] shape vec :  (7,)\n",
      "[-1  1  1 -1 -1 -1 -1] shape w :  (7,)\n",
      "[1 0 1 1 0 1 1] shape vec :  (7,)\n",
      "[ 1  1 -1  1  1 -1  1] shape w :  (7,)\n",
      "[1 0 1 1 0 1 1] shape vec :  (7,)\n",
      "[ 1  1  1  1 -1 -1  1] shape w :  (7,)\n",
      "[1 0 1 1 0 1 1] shape vec :  (7,)\n",
      "[-1  1  1 -1 -1  1  1] shape w :  (7,)\n",
      "[1 0 1 1 0 1 1] shape vec :  (7,)\n",
      "[ 1 -1  1  1 -1  1  1] shape w :  (7,)\n",
      "[1 0 1 1 0 1 1] shape vec :  (7,)\n",
      "[0 0 1 1 1 1 1] shape w :  (7,)\n",
      "[1 0 1 1 0 1 1] shape vec :  (7,)\n",
      "[ 1  1  1 -1 -1 -1 -1] shape w :  (7,)\n",
      "[1 0 1 1 0 1 1] shape vec :  (7,)\n",
      "[1 1 1 1 1 1 1] shape w :  (7,)\n",
      "[1 0 1 1 0 1 1] shape vec :  (7,)\n",
      "[ 1  1  1 -1 -1  1  1] shape w :  (7,)\n",
      "[1 0 1 1 0 1 1] shape vec :  (7,)\n",
      "Digit 5 was predicted to be 9\n",
      "[ 1  1  1  1  1  1 -1] shape w :  (7,)\n",
      "[0 0 1 1 1 1 1] shape vec :  (7,)\n",
      "[-1  1  1 -1 -1 -1 -1] shape w :  (7,)\n",
      "[0 0 1 1 1 1 1] shape vec :  (7,)\n",
      "[ 1  1 -1  1  1 -1  1] shape w :  (7,)\n",
      "[0 0 1 1 1 1 1] shape vec :  (7,)\n",
      "[ 1  1  1  1 -1 -1  1] shape w :  (7,)\n",
      "[0 0 1 1 1 1 1] shape vec :  (7,)\n",
      "[-1  1  1 -1 -1  1  1] shape w :  (7,)\n",
      "[0 0 1 1 1 1 1] shape vec :  (7,)\n",
      "[ 1 -1  1  1 -1  1  1] shape w :  (7,)\n",
      "[0 0 1 1 1 1 1] shape vec :  (7,)\n",
      "[0 0 1 1 1 1 1] shape w :  (7,)\n",
      "[0 0 1 1 1 1 1] shape vec :  (7,)\n",
      "[ 1  1  1 -1 -1 -1 -1] shape w :  (7,)\n",
      "[0 0 1 1 1 1 1] shape vec :  (7,)\n",
      "[1 1 1 1 1 1 1] shape w :  (7,)\n",
      "[0 0 1 1 1 1 1] shape vec :  (7,)\n",
      "[ 1  1  1 -1 -1  1  1] shape w :  (7,)\n",
      "[0 0 1 1 1 1 1] shape vec :  (7,)\n",
      "Digit 6 was predicted to be 9\n",
      "[ 1  1  1  1  1  1 -1] shape w :  (7,)\n",
      "[1 1 1 0 0 0 0] shape vec :  (7,)\n",
      "[-1  1  1 -1 -1 -1 -1] shape w :  (7,)\n",
      "[1 1 1 0 0 0 0] shape vec :  (7,)\n",
      "[ 1  1 -1  1  1 -1  1] shape w :  (7,)\n",
      "[1 1 1 0 0 0 0] shape vec :  (7,)\n",
      "[ 1  1  1  1 -1 -1  1] shape w :  (7,)\n",
      "[1 1 1 0 0 0 0] shape vec :  (7,)\n",
      "[-1  1  1 -1 -1  1  1] shape w :  (7,)\n",
      "[1 1 1 0 0 0 0] shape vec :  (7,)\n",
      "[ 1 -1  1  1 -1  1  1] shape w :  (7,)\n",
      "[1 1 1 0 0 0 0] shape vec :  (7,)\n",
      "[0 0 1 1 1 1 1] shape w :  (7,)\n",
      "[1 1 1 0 0 0 0] shape vec :  (7,)\n",
      "[ 1  1  1 -1 -1 -1 -1] shape w :  (7,)\n",
      "[1 1 1 0 0 0 0] shape vec :  (7,)\n",
      "[1 1 1 1 1 1 1] shape w :  (7,)\n",
      "[1 1 1 0 0 0 0] shape vec :  (7,)\n",
      "[ 1  1  1 -1 -1  1  1] shape w :  (7,)\n",
      "[1 1 1 0 0 0 0] shape vec :  (7,)\n",
      "Digit 7 was predicted to be 9\n",
      "[ 1  1  1  1  1  1 -1] shape w :  (7,)\n",
      "[1 1 1 1 1 1 1] shape vec :  (7,)\n",
      "[-1  1  1 -1 -1 -1 -1] shape w :  (7,)\n",
      "[1 1 1 1 1 1 1] shape vec :  (7,)\n",
      "[ 1  1 -1  1  1 -1  1] shape w :  (7,)\n",
      "[1 1 1 1 1 1 1] shape vec :  (7,)\n",
      "[ 1  1  1  1 -1 -1  1] shape w :  (7,)\n",
      "[1 1 1 1 1 1 1] shape vec :  (7,)\n",
      "[-1  1  1 -1 -1  1  1] shape w :  (7,)\n",
      "[1 1 1 1 1 1 1] shape vec :  (7,)\n",
      "[ 1 -1  1  1 -1  1  1] shape w :  (7,)\n",
      "[1 1 1 1 1 1 1] shape vec :  (7,)\n",
      "[0 0 1 1 1 1 1] shape w :  (7,)\n",
      "[1 1 1 1 1 1 1] shape vec :  (7,)\n",
      "[ 1  1  1 -1 -1 -1 -1] shape w :  (7,)\n",
      "[1 1 1 1 1 1 1] shape vec :  (7,)\n",
      "[1 1 1 1 1 1 1] shape w :  (7,)\n",
      "[1 1 1 1 1 1 1] shape vec :  (7,)\n",
      "[ 1  1  1 -1 -1  1  1] shape w :  (7,)\n",
      "[1 1 1 1 1 1 1] shape vec :  (7,)\n",
      "Digit 8 was predicted to be 9\n",
      "[ 1  1  1  1  1  1 -1] shape w :  (7,)\n",
      "[1 1 1 0 0 1 1] shape vec :  (7,)\n",
      "[-1  1  1 -1 -1 -1 -1] shape w :  (7,)\n",
      "[1 1 1 0 0 1 1] shape vec :  (7,)\n",
      "[ 1  1 -1  1  1 -1  1] shape w :  (7,)\n",
      "[1 1 1 0 0 1 1] shape vec :  (7,)\n",
      "[ 1  1  1  1 -1 -1  1] shape w :  (7,)\n",
      "[1 1 1 0 0 1 1] shape vec :  (7,)\n",
      "[-1  1  1 -1 -1  1  1] shape w :  (7,)\n",
      "[1 1 1 0 0 1 1] shape vec :  (7,)\n",
      "[ 1 -1  1  1 -1  1  1] shape w :  (7,)\n",
      "[1 1 1 0 0 1 1] shape vec :  (7,)\n",
      "[0 0 1 1 1 1 1] shape w :  (7,)\n",
      "[1 1 1 0 0 1 1] shape vec :  (7,)\n",
      "[ 1  1  1 -1 -1 -1 -1] shape w :  (7,)\n",
      "[1 1 1 0 0 1 1] shape vec :  (7,)\n",
      "[1 1 1 1 1 1 1] shape w :  (7,)\n",
      "[1 1 1 0 0 1 1] shape vec :  (7,)\n",
      "[ 1  1  1 -1 -1  1  1] shape w :  (7,)\n",
      "[1 1 1 0 0 1 1] shape vec :  (7,)\n",
      "Digit 9 was predicted to be 9\n"
     ]
    }
   ],
   "source": [
    "## Finally let's test our perceptrons\n",
    "# The line computing the output of each perceptron is not using bias so you need to add that as well\n",
    "# If the correct perceptron doesn't have the greatest output for one or more digits then go back and\n",
    "#  edit the weights and biases in the previous cell (or check that the vectors are implemented correctly).\n",
    "# Remember to run a cell again (CONTROL + ENTER) if you update it\n",
    "\n",
    "debug = False # Set this to True if the predictions are wrong to get a more detailed output\n",
    "\n",
    "for digit in range(10): # For each digit between 0 and 9 (range(n) gives a range (almost a list) of each number between 0 and n (excluding n)\n",
    "    vector = x[digit] # Get the correct vector representation of the digit\n",
    "    \n",
    "    outputs = [] # Create an empty list to put the perceptrons' outputs in\n",
    "    for w, b in zip(weights, biases): # For each weight and bias in the lists (zip takes two lists [x1,x2,...] [y1,y2,...] and makes a new list [(x1,y1),(x2,y2),...]) \n",
    "        \n",
    "        # CHANGE THIS LINE TO ADD BIASES AS WELL\n",
    "        print(w, \"shape w : \",numpy.shape(w))\n",
    "        print(vector, \"shape vec : \",numpy.shape(vector))\n",
    "        output = w.dot(w * vector + b) # Calculating the output of the perceptron with weight w and bias b\n",
    "        \n",
    "        outputs.append(output) # Adding the output to the list of outputs\n",
    "    prediction = outputs.index(max(outputs)) # Get prediction by taking the index of the output value with maximum input\n",
    "    \n",
    "    print(f'Digit {digit} was predicted to be {prediction}') # This is an f-string with notation f'text {variable1} more text {variable2}'\n",
    "    \n",
    "    if debug: # If debug is True\n",
    "        print(f'Outputs for all perceptrons for the digit {digit}: {outputs}')\n",
    "        print() # Add a newline for formating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 1,  1,  1,  1,  1,  1, -1]), array([-1,  1,  1, -1, -1, -1, -1]), array([ 1,  1, -1,  1,  1, -1,  1]), array([ 1,  1,  1,  1, -1, -1,  1]), array([-1,  1,  1, -1, -1,  1,  1]), array([ 1, -1,  1,  1, -1,  1,  1]), array([0, 0, 1, 1, 1, 1, 1]), array([ 1,  1,  1, -1, -1, -1, -1]), array([1, 1, 1, 1, 1, 1, 1]), array([ 1,  1,  1, -1, -1,  1,  1])]\n"
     ]
    }
   ],
   "source": [
    "print(weights)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Pytorch\n",
    "\n",
    "In order to train a model with machine learning we require one thing above all else: data\n",
    "We need data that contains information that we can use to make our models learn whatever task we have at hand.\n",
    "If we want to make a neural network play chess for example our data might be millions of board states and the recommended move and we then hope the network will learn to calculate what a good move would be for a given board state.\n",
    "In our case we want to simply learn the wegiths and biases to correctly classify the digit of a seven-segment display.\n",
    "For this task we have all the data we could possibly need: All digits and their corresponding vectors. Since we want to predict the digit based on its vector we'll use the vectors as our data and the digits as the labels.\n",
    "\n",
    "Then we need to create the network in PyTorch which is really simple as it is just a single linear neural network layer.\n",
    "This corresponds to the PyTorch model [torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) which you can have a look at in the [PyTorch documentation](https://pytorch.org/docs/stable/index.html). Whenever you encounter a new PyTorch function or class it's usually a good idea to look it up in the documentation. If you wonder if a particular machine learning feature exists in PyTorch you can also search in the documentation for it or use the more brute force method of Googling \"\\<machine learning feature\\> in pytorch\".\n",
    "\n",
    "**Excercise:** Set the correct input and ouput sizes to the network to input a seven segment display vector and output a vector of predictions for each digit. Also look through the training code to make sure you understand each step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darki\\AppData\\Local\\Temp\\ipykernel_12684\\314178716.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  data = torch.Tensor(x) # A Tensor can be created by simply giving it a nested numpy array/list of numbers as input\n"
     ]
    }
   ],
   "source": [
    "## Data and labels\n",
    "# You don't need to edit this code, we simply show you how, in this case, we create the data and labels we need\n",
    "\n",
    "# A matrix (or vector) in PyTorch is usually represented by a Tensor\n",
    "# Create a Tensor with our digit vectors\n",
    "data = torch.Tensor(x) # A Tensor can be created by simply giving it a nested numpy array/list of numbers as input\n",
    "data = data.detach() # Since we won't be changing the data during training we detach the Tensor from the computation graph\n",
    "\n",
    "# Our labels will be the expected outputs of each perceptron for each digit\n",
    "# This means we can't simply say \"5\" is our expected output, but rather that we want [0,0,0,0,0,1,0,0,0,0]\n",
    "# This is called one-hot encoding where we have a vector where the value is one at the given index and zero everywhere else\n",
    "# Since a matrix with one-hot representations of the corresponding index is simply an identity matrix, we use that as our labels\n",
    "labels = torch.eye(10) # Get a matrix with one-hot representations of each digit in each row (an identity matrix)\n",
    "labels = labels.detach() # Since we won't be changing the labels during training we detach the Tensor from the computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The network\n",
    "# We create a simple network from torch.nn.Linear\n",
    "\n",
    "input_size = 7 # WHAT IS THE SIZE OF THE INPUT VECTOR OF THE NETWORK?\n",
    "output_size = 10 # WHAT IS THE SIZE OF THE OUTPUT VECTOR OF THE NETWORK?\n",
    "\n",
    "network = torch.nn.Linear(input_size, output_size) # Creating a single linear layer of a neural network with the given input and output sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Loss: 0.07721596211194992\n",
      "Epoch 20 - Loss: 0.05581499636173248\n",
      "Epoch 30 - Loss: 0.046205416321754456\n",
      "Epoch 40 - Loss: 0.04113209247589111\n",
      "Epoch 50 - Loss: 0.03810232877731323\n",
      "Epoch 60 - Loss: 0.03609900549054146\n",
      "Epoch 70 - Loss: 0.0346536785364151\n",
      "Epoch 80 - Loss: 0.03353244066238403\n",
      "Epoch 90 - Loss: 0.032611604779958725\n",
      "Epoch 100 - Loss: 0.03182269632816315\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Training the network\n",
    "# You don't need to edit this code, we have given you the training loop to train the network\n",
    "\n",
    "epochs = 100 # How many epochs (complete runs of the data) to train for. Since our dataset is small 100 seems reasonable\n",
    "loss_function = torch.nn.MSELoss() # What function to use to calculate the loss given the prediction and labels\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=1) # Function for updating the parameters of the network based on loss\n",
    "learning_rate = 1 # How fast to optimize the network. Since our problem is quite small we can have a large learning rate, otherise 0.01 is usually standard\n",
    "\n",
    "# Create a list to keep track of how the loss changes\n",
    "losses = []\n",
    "\n",
    "# For each epoch\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # Predict for each vector what digit they represent\n",
    "    prediction = network(data)\n",
    "    \n",
    "    # Calculate the loss of the prediction by comparing to the expected output\n",
    "    loss = loss_function(prediction, labels)\n",
    "\n",
    "    # Backpropogate the loss through the network to find the gradients of all parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters along their gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    # Clear stored gradient values\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Add the loss to the total epoch loss (item() turns a PyTorch scalar into a normal Python datatype)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    #Print the epoch and loss every 10 epochs\n",
    "    if epoch % 10 == 9:\n",
    "        print(f'Epoch {epoch+1} - Loss: {loss}')\n",
    "    \n",
    "# Plot the training loss per epoch\n",
    "plt.plot(range(1,epochs+1),losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your solution**: Execute the cell below to see whether the network managed to learn the correct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit 0 was predicted to be 0\n",
      "Digit 1 was predicted to be 1\n",
      "Digit 2 was predicted to be 2\n",
      "Digit 3 was predicted to be 3\n",
      "Digit 4 was predicted to be 4\n",
      "Digit 5 was predicted to be 5\n",
      "Digit 6 was predicted to be 6\n",
      "Digit 7 was predicted to be 7\n",
      "Digit 8 was predicted to be 8\n",
      "Digit 9 was predicted to be 9\n"
     ]
    }
   ],
   "source": [
    "## Testing the trained network\n",
    "# You don't need to edit this code\n",
    "\n",
    "with torch.no_grad(): # Since we're not training we don't want to calculate the gradients for this prediction\n",
    "    prediction = network(data) # Let's make one final prediction of the data\n",
    "\n",
    "for digit in range(10):\n",
    "    print(f'Digit {digit} was predicted to be {torch.argmax(prediction[digit])}') # argmax gets the index with the greatest value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying a larger dataset\n",
    "\n",
    "While it might be redundant to use machine learning to learn the vectors of a seven-segment display, something we can already easily make a program for, there are many datasets out there for which it is difficult, if not impossible, for to program a solution for.\n",
    "In those cases having enough data might be enough for a machine learning solution to learn how to solve the problem.\n",
    "For example, I have no idea how to write a program to tell the difference between images of cats and dogs, but I could easily gather a thousand pictures of each, label them, and use that to train a neural network to figure it out for me.\n",
    "\n",
    "One such dataset is the MNIST dataset of handwritten digits. MNIST consist of 70,000 grayscale images of size 28x28 pixels each. Our goal is to train a network that can recognize what digit an image represents.\n",
    "\n",
    "The code below gets the dataset (downloads it if necessary) and displays one of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the training split of the MNIST dataset, the ToTensor makes sure we get the data as tensors, not images\n",
    "mnist_data = torchvision.datasets.MNIST('./', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "# Create a DataLoader from the dataset that we'll use to get batches of data during training\n",
    "mnist_loader = torch.utils.data.DataLoader(mnist_data, batch_size=1000, shuffle=False)\n",
    "\n",
    "# Function that takes an MNIST tensor and shows the image\n",
    "def plot_digit(data):\n",
    "    # Transfrom the images into an appropriate shape for displaying\n",
    "    data = data.view(28,28)\n",
    "    plt.imshow(data, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "# Get the first batch of images and labels from the DataLoader\n",
    "images, labels = next(iter(mnist_loader))\n",
    "\n",
    "# Plot the first image of the batch\n",
    "plot_digit(images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch\n",
    "\n",
    "In this course we will be using the deep learning framework **PyTorch**.\n",
    "This exercise will be a light introduction to the framework, but to really get to know PyTorch we recommend this tutorial [https://pytorch.org/tutorials/beginner/pytorch_with_examples.html] or finding another tutorial that suits you better.\n",
    "\n",
    "\n",
    "To solve this task you will need to improve the neural network provided below.\n",
    "This can be done by adding or changing the modules that are part of nn.Sequential. A list of available modules can be found here: [https://pytorch.org/docs/stable/nn.html] though it's recommended that you stick with nn.Linear, nn.Sigmoid, nn.ReLU, nn.LeakyReLU, or similar modules. Feed-Forward networks (the type recommended for this task, we'll tell you more about that later) usually consist of nn.Linear separated by activation functions (like nn.Sigmoid, and nn.Linear). Each nn.Linear layer must have the input size (first parameter) of the previous layer's output size. E.g. nn.Linear(784, 50) could be followed by nn.Linear(50, 10). \n",
    "\n",
    "For inspiration, see the MNIST website [http://yann.lecun.com/exdb/mnist/] which contains some previously tested network architectures and the corresponding classification accuracy obtained.\n",
    "\n",
    "Additionally, you might want to change the optimizer used; (List here: [https://pytorch.org/docs/stable/optim.html])\n",
    "or loss function (List here: [https://pytorch.org/docs/stable/nn.html#loss-functions])\n",
    "\n",
    "Note: Not all loss functions takes the same input so you might need to restructure the data to make it work.\n",
    "\n",
    "Any network that you design will be randomly initialized and therefore bad at recognizing images initially, which you will experience when running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# This code initializes the neural network\n",
    "\n",
    "### EDIT CODE BELOW TO CHANGE THE NETWORK AND ITS OPTIMIZING PROCEDURE (AFTER TRAINING AND PREDICTING THE FIRST TIME) ###\n",
    "# nn.Sequential can be given a list of neural networks modules\n",
    "\n",
    "# This initial network has only two layers with a ReLU funtion in between.\n",
    "# It has an input size equal to the size of the images (28x28 pixels = 784)\n",
    "# and an output size equal to the number of classes (the number of digits = 10)\n",
    "network = nn.Sequential(\n",
    "    nn.Linear(784, 100), # First layer of the network takes the entire image and reduces it to 100 dimensions\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 10) # The second layer takes those 100 dimensions and reduces them into estimated values for each digit\n",
    ")\n",
    "\n",
    "# Initialize the optimizer\n",
    "# In addition to changing optimizer you can try to change other parameters like learning rate (lr)\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "### EDIT CODE ABOVE TO CHANGE THE NETWORK AND ITS OPTIMIZING PROCEDURE ###\n",
    "\n",
    "# An Embedding layer used for turning int into one-hot (0 -> [1,0,0,0,0,0,0,0,0,0], 5 -> [0,0,0,0,0,1,0,0,0,0])\n",
    "to_onehot = nn.Embedding(10, 10) \n",
    "to_onehot.weight.data = torch.eye(10)\n",
    "\n",
    "# Extract some images from the dataset and have the new predict what digit they are\n",
    "def predict_on_images(iterator, images_to_show):\n",
    "    for index in range(images_to_show):\n",
    "        # Get the next batch of images\n",
    "        images, labels = next(iterator)\n",
    "\n",
    "        plot_digit(images[0])\n",
    "\n",
    "        # Transform the images into a single list of pixels since our network takes that as its input\n",
    "        input_tensor = images[0].view(1,784)\n",
    "        # Run the input through our network to get a prediction\n",
    "        prediction = network(input_tensor)\n",
    "        # Extract which prediction had the highest probability\n",
    "        guess = torch.argmax(prediction[0], dim=-1)\n",
    "        # Show the predicted digit and the actual digit\n",
    "        print(f'Prediction: {guess.item()} - Actual: {labels[0].item()}')\n",
    "\n",
    "# Have the untrained network predict on some images\n",
    "print('Predicting with the randomly initialized network before training to see what happens')\n",
    "predict_on_images(iterator = iter(mnist_loader), images_to_show = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network\n",
    "Below is the training procedure. Neural networks are usually trained with backpropagation which works as follows (for now you don't need to understand the prodcedure, but you'll be expected to learn it in the course later on).\n",
    "    1. Give the network the input and have it calculate a prediction.\n",
    "    2. Calculate the loss/error by comparing the difference between the prediction and the target output.\n",
    "    3. For the error E and each parameter w find their error gradient: (backpropagate the error)\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial E}{\\partial w}\n",
    "\\end{equation*}\n",
    "    4. Update the parameters according to their error gradient (since we know how this parameter affected the error we can change it to cause less error)\n",
    "    5. Repeat from step 1 with new input\n",
    "    \n",
    "The code below **takes a while to execute**. Note the circle at the top-right corner of the notebook, just next to the name Python followed by the version number, like **Python 3**. When the circle is filled the code is still running. An open circle means that code is not running.\n",
    "\n",
    "**Exercise:** Complete the training loop below and train the network. Then go back to where the network and other training parameters are defined and change things around and see if you can train with some other network architectures, loss functions, or optimizers.\n",
    "\n",
    "*Hint:* Look at the training loop from the seven segment training to see how different parts are implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 [42/60] - Loss: 0.019569780677556996"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 8\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# For each epoch\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m      6\u001B[0m     \n\u001B[0;32m      7\u001B[0m     \u001B[38;5;66;03m# For each batch of data (since the dataset is too large to run all data through the network at once)\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m batch_nr, (images, labels) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(mnist_loader):\n\u001B[0;32m      9\u001B[0m         \n\u001B[0;32m     10\u001B[0m         \u001B[38;5;66;03m# Extract the labels and turn them into one-hot representation (note: not all loss functions needs this)\u001B[39;00m\n\u001B[0;32m     11\u001B[0m         labels \u001B[38;5;241m=\u001B[39m to_onehot(labels)\n\u001B[0;32m     13\u001B[0m         \u001B[38;5;66;03m# Reshape the images to a single vector (28*28 = 784)\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\Skol dokument\\D7046E\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\Documents\\Skol dokument\\D7046E\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    670\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 671\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    672\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    673\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\Documents\\Skol dokument\\D7046E\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\Documents\\Skol dokument\\D7046E\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\Documents\\Skol dokument\\D7046E\\venv\\lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001B[0m, in \u001B[0;36mMNIST.__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m    142\u001B[0m img \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mfromarray(img\u001B[38;5;241m.\u001B[39mnumpy(), mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mL\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    144\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 145\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    147\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    148\u001B[0m     target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform(target)\n",
      "File \u001B[1;32m~\\Documents\\Skol dokument\\D7046E\\venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:135\u001B[0m, in \u001B[0;36mToTensor.__call__\u001B[1;34m(self, pic)\u001B[0m\n\u001B[0;32m    127\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, pic):\n\u001B[0;32m    128\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    129\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m    130\u001B[0m \u001B[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    133\u001B[0m \u001B[38;5;124;03m        Tensor: Converted image.\u001B[39;00m\n\u001B[0;32m    134\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 135\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Skol dokument\\D7046E\\venv\\lib\\site-packages\\torchvision\\transforms\\functional.py:167\u001B[0m, in \u001B[0;36mto_tensor\u001B[1;34m(pic)\u001B[0m\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pic\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    166\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m255\u001B[39m \u001B[38;5;241m*\u001B[39m img\n\u001B[1;32m--> 167\u001B[0m img \u001B[38;5;241m=\u001B[39m \u001B[43mimg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpic\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetbands\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    168\u001B[0m \u001B[38;5;66;03m# put it from HWC to CHW format\u001B[39;00m\n\u001B[0;32m    169\u001B[0m img \u001B[38;5;241m=\u001B[39m img\u001B[38;5;241m.\u001B[39mpermute((\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m))\u001B[38;5;241m.\u001B[39mcontiguous()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Decide the number of epochs to train for (one epoch is one optimization iteration on the entire dataset)\n",
    "epochs = 20\n",
    "\n",
    "# For each epoch\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # For each batch of data (since the dataset is too large to run all data through the network at once)\n",
    "    for batch_nr, (images, labels) in enumerate(mnist_loader):\n",
    "        \n",
    "        # Extract the labels and turn them into one-hot representation (note: not all loss functions needs this)\n",
    "        labels = to_onehot(labels)\n",
    "        \n",
    "        # Reshape the images to a single vector (28*28 = 784)\n",
    "        images = images.view(-1,784)\n",
    "        \n",
    "        # Predict for each digit in the batch what class they belong to\n",
    "        prediction = network(images) # WRITE THE CODE TO RUN THE IMAGES THROUGH THE NETWORK\n",
    "        \n",
    "        # Calculate the loss of the prediction by comparing to the expected output\n",
    "        loss = loss_function(prediction, labels) # WRITE THE CODE TO CALCULATE THE LOSS BY COMPARING THE PREDICTION TO THE ACTUAL LABELS\n",
    "        \n",
    "        # Backpropagate the loss through the network to find the gradients of all parameters\n",
    "        loss.backward() # WRITE CODE TO BACKPROPOGATE THE LOSS\n",
    "        \n",
    "        # Update the parameters along their gradients\n",
    "        optimizer.step() # WRITE THE CODE TO OPTIMIZE PARAMETERS BY STEPPING ALONG THE GRADIENTS\n",
    "        \n",
    "        # Clear stored gradient values\n",
    "        optimizer.zero_grad() # WRITE THE CODE TO REMOVE THE USED GRADIENTS BEFORE THE NEXT ITERATION OF THE LOOP\n",
    "        \n",
    "        #Print the epoch, batch, and loss\n",
    "        print(\n",
    "            '\\rEpoch {} [{}/{}] - Loss: {}'.format(\n",
    "                epoch+1, batch_nr+1, len(mnist_loader), loss\n",
    "            ),\n",
    "            end=''\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the network\n",
    "To see whether our network have been trained properly we want to test it on new data, which has not been used during training. In Exercise 1 you'll need to implement a proper testing procedure, but for now we simplify things and just predict a number of digits to see whether it looks fairly alright.\n",
    "\n",
    "Likely the network will not get everyhting correct, but that's to be expected with the few epochs of training and non-optimal setup.\n",
    "\n",
    "**Exercise:** Can you improve the network architecture to improve the classification results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have the trained network predict on a number of images\n",
    "predict_on_images(iterator = iter(mnist_loader), images_to_show = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
